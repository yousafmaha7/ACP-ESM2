{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ee3b8b-8bf5-4784-9efa-4da45df02043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: D:\\ACP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set working directory\n",
    "os.chdir(\"D:/ACP\")  # Windows path fix: use forward slash or raw string\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdcb9bcb-bfa6-4065-a1ad-26d5415896c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\myousaf23\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\myousaf23\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Required Libraries (run once)\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9d6712-8e1b-40ad-8f6f-654cfa1ce6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                               Sequence\n",
      "0  ACP_335                      GTGLPMSERRKIMLMMR\n",
      "1  ACP_580                   FLSLALAALPKLFCLIFKKC\n",
      "2   ACP_74                          FLPIITNLLGKLL\n",
      "3  ACP_428                          EGGGPQWAVGHFM\n",
      "4   ACP_79  DQYKCLQHGGFCLRSSCPSNTKLQGTCKPDKPNCCKS\n",
      "FASTA file saved to: acp_train_data.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myousaf23\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Extracting BERT features:   0%|                                                                | 0/738 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Extracting BERT features: 100%|██████████████████████████████████████████████████████| 738/738 [01:23<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to acp_train_bert_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import Libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 3: Load Your Peptide CSV File\n",
    "csv_path = \"acp_train_data.csv\"  # change this to your actual path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Confirm structure: should have 'ID' and 'Sequence' columns\n",
    "print(df[['ID', 'Sequence']].head())\n",
    "\n",
    "# Step 4: Convert to FASTA Format and Save\n",
    "fasta_path = \"acp_train_data.fasta\"\n",
    "with open(fasta_path, \"w\") as fasta_file:\n",
    "    for idx, row in df.iterrows():\n",
    "        fasta_file.write(f\">{row['ID']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "print(f\"FASTA file saved to: {fasta_path}\")\n",
    "\n",
    "# Step 5: Preprocess Sequences for ProtBERT\n",
    "def preprocess_sequence(seq):\n",
    "    return \" \".join(list(seq))  # adds spaces between amino acids\n",
    "\n",
    "sequences = df[\"Sequence\"].apply(preprocess_sequence).tolist()\n",
    "ids = df[\"ID\"].tolist()\n",
    "\n",
    "# Step 6: Load ProtBERT Model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model.eval()\n",
    "\n",
    "# Step 7: Feature Extraction Function\n",
    "def extract_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # mean pooling\n",
    "    return embedding\n",
    "\n",
    "# Step 8: Apply to All Sequences\n",
    "features = []\n",
    "for seq in tqdm(sequences, desc=\"Extracting BERT features\"):\n",
    "    vec = extract_embedding(seq)\n",
    "    features.append(vec)\n",
    "\n",
    "# Step 9: Create DataFrame with Features\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.insert(0, \"ID\", ids)\n",
    "features_df[\"Label\"] = df[\"Label\"]\n",
    "\n",
    "# Step 10: Save Feature CSV\n",
    "features_df.to_csv(\"acp_train_bert_features.csv\", index=False)\n",
    "print(\"Saved to acp_train_bert_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34c2825-c557-4cc8-ae06-a74bf0fb9ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                         Sequence\n",
      "0    ACP_4                    FAKFLAKFLKKAL\n",
      "1  ACP_766                     LNPDPCKPLAFI\n",
      "2  ACP_870             TCGTCCTGAGGAGAGAGAGC\n",
      "3  ACP_581                    FLSGIVGMLGKLF\n",
      "4  ACP_445  GFFSTVKNLATNVAGTVIDTLKCKVTGGCRS\n",
      "FASTA file saved to: acp_test_data.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myousaf23\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Extracting BERT features:   0%|                                                                | 0/185 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Extracting BERT features: 100%|██████████████████████████████████████████████████████| 185/185 [00:23<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to acp_test_bert_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load Your Peptide CSV File\n",
    "csv_path = \"acp_test_data.csv\"  # change this to your actual path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Confirm structure: should have 'ID' and 'Sequence' columns\n",
    "print(df[['ID', 'Sequence']].head())\n",
    "\n",
    "# Step 4: Convert to FASTA Format and Save\n",
    "fasta_path = \"acp_test_data.fasta\"\n",
    "with open(fasta_path, \"w\") as fasta_file:\n",
    "    for idx, row in df.iterrows():\n",
    "        fasta_file.write(f\">{row['ID']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "print(f\"FASTA file saved to: {fasta_path}\")\n",
    "\n",
    "# Step 5: Preprocess Sequences for ProtBERT\n",
    "def preprocess_sequence(seq):\n",
    "    return \" \".join(list(seq))  # adds spaces between amino acids\n",
    "\n",
    "sequences = df[\"Sequence\"].apply(preprocess_sequence).tolist()\n",
    "ids = df[\"ID\"].tolist()\n",
    "\n",
    "# Step 6: Load ProtBERT Model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "model.eval()\n",
    "\n",
    "# Step 7: Feature Extraction Function\n",
    "def extract_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # mean pooling\n",
    "    return embedding\n",
    "\n",
    "# Step 8: Apply to All Sequences\n",
    "features = []\n",
    "for seq in tqdm(sequences, desc=\"Extracting BERT features\"):\n",
    "    vec = extract_embedding(seq)\n",
    "    features.append(vec)\n",
    "\n",
    "# Step 9: Create DataFrame with Features\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.insert(0, \"ID\", ids)\n",
    "features_df[\"Label\"] = df[\"Label\"]\n",
    "\n",
    "# Step 10: Save Feature CSV\n",
    "features_df.to_csv(\"acp_test_bert_features.csv\", index=False)\n",
    "print(\"Saved to acp_test_bert_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918a44a9-975a-4391-a10f-5224482f6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA file written.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to C:\\Users\\myousaf23/.cache\\torch\\hub\\checkpoints\\esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to C:\\Users\\myousaf23/.cache\\torch\\hub\\checkpoints\\esm2_t33_650M_UR50D-contact-regression.pt\n",
      "Extracting ESM embeddings: 100%|███████████████████████████████████████████████████████| 93/93 [01:51<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file saved as acp_train_esm2_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load CSV with 'ID' and 'Sequence'\n",
    "csv_path = \"acp_train_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Write FASTA file for ESM input\n",
    "fasta_path = \"acp_train_data.fasta\"\n",
    "with open(fasta_path, \"w\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        f.write(f\">{row['ID']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "print(\"FASTA file written.\")\n",
    "\n",
    "# Step 3: Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()  # or esm1b_t33_650M_UR50S\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "# Step 4: Prepare data\n",
    "data = [(row['ID'], row['Sequence']) for _, row in df.iterrows()]\n",
    "\n",
    "# Step 5: Extract embeddings\n",
    "results = []\n",
    "batch_size = 8  # adjust based on GPU/CPU RAM\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Extracting ESM embeddings\"):\n",
    "    batch_data = data[i:i+batch_size]\n",
    "    labels, strs, tokens = batch_converter(batch_data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, repr_layers=[33])\n",
    "        token_representations = outputs[\"representations\"][33]\n",
    "    \n",
    "    for j, (label, seq) in enumerate(batch_data):\n",
    "        # Ignore [CLS] (token 0) and [EOS] (last token)\n",
    "        embedding = token_representations[j, 1:len(seq)+1].mean(0).cpu().numpy()\n",
    "        results.append([label] + embedding.tolist())\n",
    "\n",
    "# Step 6: Save to CSV\n",
    "embedding_dim = len(results[0]) - 1\n",
    "columns = [\"ID\"] + [f\"feat_{i}\" for i in range(embedding_dim)]\n",
    "features_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Add labels from original CSV\n",
    "features_df = features_df.merge(df[['ID', 'Label']], on='ID')\n",
    "\n",
    "# Step 7: Save\n",
    "features_df.to_csv(\"acp_train_esm2_features.csv\", index=False)\n",
    "print(\"Feature file saved as acp_train_esm2_features.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1031c716-916b-4308-ad0c-7aa1c5110c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA file written.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ESM embeddings: 100%|███████████████████████████████████████████████████████| 24/24 [00:28<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file saved as acp_test_esm2_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load CSV with 'ID' and 'Sequence'\n",
    "csv_path = \"acp_test_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Write FASTA file for ESM input\n",
    "fasta_path = \"acp_test_data.fasta\"\n",
    "with open(fasta_path, \"w\") as f:\n",
    "    for i, row in df.iterrows():\n",
    "        f.write(f\">{row['ID']}\\n{row['Sequence']}\\n\")\n",
    "\n",
    "print(\"FASTA file written.\")\n",
    "\n",
    "# Step 3: Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()  # or esm1b_t33_650M_UR50S\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "# Step 4: Prepare data\n",
    "data = [(row['ID'], row['Sequence']) for _, row in df.iterrows()]\n",
    "\n",
    "# Step 5: Extract embeddings\n",
    "results = []\n",
    "batch_size = 8  # adjust based on GPU/CPU RAM\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size), desc=\"Extracting ESM embeddings\"):\n",
    "    batch_data = data[i:i+batch_size]\n",
    "    labels, strs, tokens = batch_converter(batch_data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, repr_layers=[33])\n",
    "        token_representations = outputs[\"representations\"][33]\n",
    "    \n",
    "    for j, (label, seq) in enumerate(batch_data):\n",
    "        # Ignore [CLS] (token 0) and [EOS] (last token)\n",
    "        embedding = token_representations[j, 1:len(seq)+1].mean(0).cpu().numpy()\n",
    "        results.append([label] + embedding.tolist())\n",
    "\n",
    "# Step 6: Save to CSV\n",
    "embedding_dim = len(results[0]) - 1\n",
    "columns = [\"ID\"] + [f\"feat_{i}\" for i in range(embedding_dim)]\n",
    "features_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Add labels from original CSV\n",
    "features_df = features_df.merge(df[['ID', 'Label']], on='ID')\n",
    "\n",
    "# Step 7: Save\n",
    "features_df.to_csv(\"acp_test_esm2_features.csv\", index=False)\n",
    "print(\"Feature file saved as acp_test_esm2_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5cdf5-4187-4ca2-aba9-e2f1321415c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
